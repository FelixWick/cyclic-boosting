{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression using Cyclic Boosting with High cardinality data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cyclic Boosting performs exceptionally well in cases with high cardinality, which refers to the presence of many unique values in a dataset, such as product IDs or user IDs. High cardinality can make regression tasks challenging because it complicates the modeling process, potentially leading to overfitting or inefficiencies in capturing patterns. However, unlike most other regression methods, Cyclic Boosting conducts local optimization for each individual feature bin (which means each individual category in the case of categorical features), making it a very low-bias approach.\n",
    "\n",
    "Examples of high cardinality:\n",
    "- Sales Forecasting: Forecasting sales using product IDs or customer IDs in retail and e-commerce.\n",
    "\n",
    "- Ad Delivery: Predicting advertising effectiveness and click-through rates based on user IDs and device IDs.\n",
    "\n",
    "- Financial Analysis: Analyzing credit scores and loan default risk using customer accounts and transaction IDs.\n",
    "\n",
    "- Medical Analysis: Predicting medical costs and treatment effects based on patient IDs.\n",
    "\n",
    "- Social Media Analysis: Predicting engagement using post and user IDs.\n",
    "\n",
    "- Traffic Prediction: Forecasting traffic volume and flight delays using vehicle IDs and operation records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional formatting if juypter-black is installed\n",
    "try:\n",
    "    import jupyter_black\n",
    "\n",
    "    jupyter_black.load(line_length=80)\n",
    "except ImportError:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "from cyclic_boosting import flags, common_smoothers, observers\n",
    "from cyclic_boosting.pipelines import pipeline_CBPoissonRegressor\n",
    "\n",
    "from cyclic_boosting.smoothing.onedim import SeasonalSmoother, IsotonicRegressor\n",
    "\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load high cardinality datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will conduct experiments using three simulated sales datasets with varying degrees of high cardinality for product IDs.  \n",
    "The 3 datasets have been simulated such that the proportion of unique product IDs relative to the total number of records is low: 25%, moderate: 40%, and extreme: 56%. The test set for each dataset was also simulated varying only the data period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "url_dict = {\n",
    "    \"low\": \"https://raw.githubusercontent.com/Blue-Yonder-OSS/cyclic-boosting/main/tests/high_cardinality_data/high_cardinality_data_low.csv\",\n",
    "    \"low_test\": \"https://raw.githubusercontent.com/Blue-Yonder-OSS/cyclic-boosting/main/tests/high_cardinality_data/high_cardinality_data_low_test.csv\",\n",
    "    \"moderate\": \"https://raw.githubusercontent.com/Blue-Yonder-OSS/cyclic-boosting/main/tests/high_cardinality_data/high_cardinality_data_moderate.csv\",\n",
    "    \"moderate_test\": \"https://raw.githubusercontent.com/Blue-Yonder-OSS/cyclic-boosting/main/tests/high_cardinality_data/high_cardinality_data_moderate_test.csv\",\n",
    "    \"extreme\": \"https://raw.githubusercontent.com/Blue-Yonder-OSS/cyclic-boosting/main/tests/high_cardinality_data/high_cardinality_data_extreme.csv\",\n",
    "    \"extreme_test\": \"https://raw.githubusercontent.com/Blue-Yonder-OSS/cyclic-boosting/main/tests/high_cardinality_data/high_cardinality_data_extreme_test.csv\",\n",
    "}\n",
    "\n",
    "for level, url in url_dict.items():\n",
    "    urllib.request.urlretrieve(url, f\"high_cardinality_data_{level}.csv\")\n",
    "\n",
    "df_low = pd.read_csv(\"high_cardinality_data_low.csv\")\n",
    "df_low_test = pd.read_csv(\"high_cardinality_data_low_test.csv\")\n",
    "df_mod = pd.read_csv(\"high_cardinality_data_moderate.csv\")\n",
    "df_mod_test = pd.read_csv(\"high_cardinality_data_moderate_test.csv\")\n",
    "df_ext = pd.read_csv(\"high_cardinality_data_extreme.csv\")\n",
    "df_ext_test = pd.read_csv(\"high_cardinality_data_extreme_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize cardinality, we will calculate the cardinality ratio for each dataset, which is the ratio of unique values in the column (in this case, the P_ID column) to the total number of data points. It is commonly accepted that if this ratio exceeds 20%, the column is considered to have high cardinality. Additionally, we will calculate the number of records contained for each product within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_datasets = pd.DataFrame(\n",
    "    index=[\"low\", \"moderate\", \"extreme\"],\n",
    "    columns=[\n",
    "        \"n_unique_products\",\n",
    "        \"n_records\",\n",
    "        \"cardinality_ratio\",\n",
    "        \"n_records_per_product_mean\",\n",
    "        \"n_records_per_product_var\",\n",
    "    ],\n",
    ")\n",
    "for level, df in {\n",
    "    \"low\": df_low,\n",
    "    \"low_test\": df_low_test,\n",
    "    \"moderate\": df_mod,\n",
    "    \"moderate_test\": df_mod_test,\n",
    "    \"extreme\": df_ext,\n",
    "    \"extreme_test\": df_ext_test,\n",
    "}.items():\n",
    "    cardinality_ratio = df[\"P_ID\"].nunique() / len(df)\n",
    "    n_records_per_product_mean = df[\"P_ID\"].value_counts().mean()\n",
    "    n_records_per_product_var = df[\"P_ID\"].value_counts().var()\n",
    "    df_datasets.loc[level] = [\n",
    "        df[\"P_ID\"].nunique(),\n",
    "        len(df),\n",
    "        cardinality_ratio,\n",
    "        n_records_per_product_mean,\n",
    "        n_records_per_product_var,\n",
    "    ]\n",
    "\n",
    "display(df_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "Here, we will display the distribution of \"SALES\" for each dataset and the distribution of the number of records for each product.\n",
    "\n",
    "The left histogram depicts the frequency distribution of sales values. The x-axis represents different sales values. The y-axis indicates how frequently each sales value occurs, with a logarithmic scale. The histogram shows that the majority of sales values are low, with a steep decrease in frequency as sales values increase. This type of distribution is common in sales data, where a few high sales values are less frequent compared to many low sales values.\n",
    "\n",
    "The right histogram displays the count of unique product ID (P_ID) with specific number of records. The x-axis shows the number of records, and the y-axis indicates the number of products having that specific number of records, also with a logarithmic scale. It means that total count of y-axis is number of total unique products. The histogram suggests that most product IDs have a lower number of records, and as the number of records increases, the frequency of such products decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for level, df in {\n",
    "    \"low\": df_low,\n",
    "    \"moderate\": df_mod,\n",
    "    \"extreme\": df_ext,\n",
    "}.items():\n",
    "    fig, [ax1, ax2] = plt.subplots(1, 2, figsize=(12, 3))\n",
    "    fig.suptitle(f\"{level} high cardinality data\")\n",
    "    ax1.hist(df[\"SALES\"], log=True, bins=30, range=(0, 30))\n",
    "    ax1.set_title(\"SALES\")\n",
    "    ax1.set_xlabel(\"SALES\")\n",
    "    ax1.set_ylabel(\"Frequency\")\n",
    "    ax2.hist(df[\"P_ID\"].value_counts(), log=True, bins=25, range=(0, 25))\n",
    "    ax2.set_title(\"n_record per P_ID\")\n",
    "    ax2.set_xlabel(\"n_record\")\n",
    "    ax2.set_ylabel(\"n_product\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df_train, df_test):\n",
    "    for key, df in {\"train\": df_train, \"test\": df_test}.items():\n",
    "        df[\"DATE\"] = pd.to_datetime(df[\"DATE\"])\n",
    "        df[\"dayofweek\"] = df[\"DATE\"].dt.dayofweek\n",
    "        df[\"dayofyear\"] = df[\"DATE\"].dt.dayofyear\n",
    "\n",
    "        df = df.drop(columns=[\"LAMBDA\"])\n",
    "\n",
    "        df[\"price_ratio\"] = df[\"SALES_PRICE\"] / df[\"NORMAL_PRICE\"]\n",
    "        df[\"price_ratio\"].fillna(1, inplace=True)\n",
    "        df[\"price_ratio\"].clip(0, 1, inplace=True)\n",
    "        df.loc[df[\"price_ratio\"] == 1.0, \"price_ratio\"] = np.nan\n",
    "\n",
    "        enc = OrdinalEncoder(\n",
    "            handle_unknown=\"use_encoded_value\", unknown_value=np.nan\n",
    "        )\n",
    "        df[\n",
    "            [\"P_ID\", \"PG_ID_1\", \"PG_ID_2\", \"PG_ID_3\", \"EVENT\"]\n",
    "        ] = enc.fit_transform(\n",
    "            df[[\"P_ID\", \"PG_ID_1\", \"PG_ID_2\", \"PG_ID_3\", \"EVENT\"]]\n",
    "        )\n",
    "\n",
    "        y = np.asarray(df[\"SALES\"])\n",
    "        X = df.drop(columns=\"SALES\")\n",
    "        X = X.drop(columns=\"DATE\")\n",
    "\n",
    "        if key == \"train\":\n",
    "            X_train = X.copy()\n",
    "            y_train = np.copy(y)\n",
    "        else:\n",
    "            X_test = X.copy()\n",
    "            y_test = np.copy(y)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the feature properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_properties = {\n",
    "    \"P_ID\": flags.IS_UNORDERED,\n",
    "    \"PG_ID_1\": flags.IS_UNORDERED,\n",
    "    \"PG_ID_2\": flags.IS_UNORDERED,\n",
    "    \"PG_ID_3\": flags.IS_UNORDERED,\n",
    "    \"dayofweek\": flags.IS_ORDERED,\n",
    "    \"dayofyear\": flags.IS_CONTINUOUS | flags.IS_LINEAR,\n",
    "    \"SALES_PRICE\": flags.IS_CONTINUOUS,\n",
    "    \"NORMAL_PRICE\": flags.IS_CONTINUOUS,\n",
    "    \"price_ratio\": flags.IS_CONTINUOUS\n",
    "    | flags.HAS_MISSING\n",
    "    | flags.MISSING_NOT_LEARNED,\n",
    "    \"PROMOTION_TYPE\": flags.IS_ORDERED,\n",
    "    \"SCHOOL_HOLIDAY\": flags.IS_UNORDERED,\n",
    "    \"EVENT\": flags.IS_UNORDERED | flags.HAS_MISSING,\n",
    "}\n",
    "\n",
    "features = [\n",
    "    \"P_ID\",\n",
    "    \"PG_ID_1\",\n",
    "    \"PG_ID_2\",\n",
    "    \"PG_ID_3\",\n",
    "    \"dayofweek\",\n",
    "    \"dayofyear\",\n",
    "    \"SALES_PRICE\",\n",
    "    \"NORMAL_PRICE\",\n",
    "    \"price_ratio\",\n",
    "    \"PROMOTION_TYPE\",\n",
    "    \"SCHOOL_HOLIDAY\",\n",
    "    \"EVENT\",\n",
    "    # interaction terms\n",
    "    (\"P_ID\", \"dayofyear\"),\n",
    "    (\"P_ID\", \"dayofweek\"),\n",
    "    (\"P_ID\", \"SALES_PRICE\"),\n",
    "    (\"PG_ID_1\", \"PROMOTION_TYPE\"),\n",
    "    (\"PG_ID_2\", \"PROMOTION_TYPE\"),\n",
    "    (\"PG_ID_3\", \"PROMOTION_TYPE\"),\n",
    "    (\"PG_ID_1\", \"dayofweek\"),\n",
    "    (\"PG_ID_2\", \"dayofweek\"),\n",
    "    (\"PG_ID_3\", \"dayofweek\"),\n",
    "    (\"PG_ID_1\", \"dayofyear\"),\n",
    "    (\"PG_ID_2\", \"dayofyear\"),\n",
    "    (\"PG_ID_3\", \"dayofyear\"),\n",
    "    (\"PG_ID_1\", \"SALES_PRICE\"),\n",
    "    (\"PG_ID_2\", \"SALES_PRICE\"),\n",
    "    (\"PG_ID_3\", \"SALES_PRICE\"),\n",
    "    (\"PG_ID_1\", \"PG_ID_2\", \"PG_ID_3\"),\n",
    "    (\"price_ratio\", \"PROMOTION_TYPE\"),\n",
    "    (\"dayofweek\", \"PROMOTION_TYPE\"),\n",
    "    (\"dayofyear\", \"PROMOTION_TYPE\"),\n",
    "    (\"EVENT\", \"PROMOTION_TYPE\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cb_poisson_regressor_model():\n",
    "    explicit_smoothers = {\n",
    "        (\"dayofyear\",): SeasonalSmoother(order=3),\n",
    "        (\"price_ratio\",): IsotonicRegressor(increasing=False),\n",
    "    }\n",
    "\n",
    "    plobs = [\n",
    "        observers.PlottingObserver(iteration=1),\n",
    "        observers.PlottingObserver(iteration=-1),\n",
    "    ]\n",
    "\n",
    "    CB_pipeline = pipeline_CBPoissonRegressor(\n",
    "        feature_properties=feature_properties,\n",
    "        feature_groups=features,\n",
    "        observers=plobs,\n",
    "        maximal_iterations=50,\n",
    "        smoother_choice=common_smoothers.SmootherChoiceGroupBy(\n",
    "            use_regression_type=True,\n",
    "            use_normalization=False,\n",
    "            explicit_smoothers=explicit_smoothers,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return CB_pipeline\n",
    "\n",
    "\n",
    "cb_poisson_regressor_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and prediction\n",
    "\n",
    "For comparison, we will perform regression using Cyclic Boosting and Light GBM. (You need to install the lightgbm package before running.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_predict_CB(df_train, df_test):\n",
    "    X_train, X_test, y_train, y_test = prepare_data(df_train, df_test)\n",
    "    CB_est = cb_poisson_regressor_model()\n",
    "    _ = CB_est.fit(X_train.copy(), y_train)\n",
    "    yhat = CB_est.predict(X_test.copy())\n",
    "    mae = np.nanmean(np.abs(y_test - yhat))\n",
    "    smape = (\n",
    "        np.mean(np.abs(y_test - yhat) / (np.abs(y_test) + np.abs(yhat))) * 100\n",
    "    )\n",
    "    return mae, smape\n",
    "\n",
    "\n",
    "def fit_and_predict_LGBM(df_train, df_test):\n",
    "    X_train, X_test, y_train, y_test = prepare_data(df_train, df_test)\n",
    "    params = {\"objective\": \"poisson\", \"verbosity\": -1}\n",
    "    lgb_train = lgb.Dataset(X_train, y_train)\n",
    "    model = lgb.train(params=params, train_set=lgb_train)\n",
    "    yhat = model.predict(X_test)\n",
    "    mae = np.nanmean(np.abs(y_test - yhat))\n",
    "    smape = (\n",
    "        np.mean(np.abs(y_test - yhat) / (np.abs(y_test) + np.abs(yhat))) * 100\n",
    "    )\n",
    "    return mae, smape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the accuracy of Cyclic Boosting and Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = [\"CB\", \"LGBM\", \"defference\"]\n",
    "columns = [\"low\", \"moderate\", \"extreme\"]\n",
    "df_mae = pd.DataFrame(index=index, columns=columns)\n",
    "df_smape = pd.DataFrame(index=index, columns=columns)\n",
    "\n",
    "for level, dfs in {\n",
    "    \"low\": {\"df_train\": df_low, \"df_test\": df_low_test},\n",
    "    \"moderate\": {\"df_train\": df_mod, \"df_test\": df_mod_test},\n",
    "    \"extreme\": {\"df_train\": df_ext, \"df_test\": df_ext_test},\n",
    "}.items():\n",
    "    print(f\"-- Processing {level} high cardinality data\")\n",
    "    mae_CB, smape_CB = fit_and_predict_CB(**dfs)\n",
    "    mae_LGBM, smape_LGBM = fit_and_predict_LGBM(**dfs)\n",
    "    df_mae[level] = [mae_CB, mae_LGBM, mae_LGBM - mae_CB]\n",
    "    df_smape[level] = [smape_CB, smape_LGBM, smape_LGBM - smape_CB]\n",
    "\n",
    "display(df_mae, df_smape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
